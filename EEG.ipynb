{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342e73f1-7902-4177-87b2-e23583e820d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export CXX=g++-8 CC=gcc-8\n",
    "# pip install torcheeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef5decbf-2b18-40aa-b961-d40d761bf3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torcheeg.models import ATCNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd \n",
    "import typing as tp\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "272d4e95-e7fd-4851-bba9-cc3d46b20b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b471b24-a404-47b6-a605-e06b653c11f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "FilePath = tp.Union[str, Path]\n",
    "Label = tp.Union[int, float, np.ndarray]\n",
    "from tqdm import tqdm \n",
    "CLASSES = [\"seizure_vote\", \"lpd_vote\", \"gpd_vote\", \"lrda_vote\", \"grda_vote\", \"other_vote\"]\n",
    "N_CLASSES = len(CLASSES)\n",
    "\n",
    "\n",
    "class HMSHBACSpecDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_paths: tp.Sequence[FilePath],\n",
    "        labels: tp.Sequence[Label]\n",
    "    ):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        \n",
    "        #load em all cowboy \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        label = self.labels[index]\n",
    "        eeg = pd.read_parquet(self.image_paths[index])\n",
    "        rows = len(eeg)\n",
    "        offset = (rows-10_000)//2\n",
    "        eeg = eeg.iloc[offset:offset+10_000]\n",
    "        eeg = eeg[['Fp1', 'O1', 'Fp2', 'O2']]\n",
    "        for j, col in enumerate(eeg.columns ) : \n",
    "            x = eeg[col].values.astype('float32')\n",
    "            m = np.nanmean(x)\n",
    "            if np.isnan(x).mean()<1:\n",
    "                x = np.nan_to_num(x,nan=m)\n",
    "            else: \n",
    "                x[:] = 0\n",
    "\n",
    "            eeg[col] = x\n",
    "\n",
    "        eeg = torch.tensor(eeg.values).T.unsqueeze(0)\n",
    "\n",
    "        return eeg, label \n",
    "    \n",
    "def get_path_label( train_all: pd.DataFrame):\n",
    "    \"\"\"Get file path and target info.\"\"\"\n",
    "    \n",
    "    train, test = train_test_split(train_all, test_size=0.2,  stratify=train_all[CLASSES].values.argmax(axis=1)  )\n",
    "                                        \n",
    "    train_paths = []\n",
    "    test_paths = [] \n",
    "                                                                 \n",
    "    trainlabels = train[CLASSES].values                                               \n",
    "    for label_id in train[\"eeg_id\"].values:\n",
    "        img_path = f\"train_eegs/{label_id}.parquet\"\n",
    "        train_paths.append(img_path)\n",
    "                             \n",
    "    testlabels = test[CLASSES].values                                               \n",
    "    for label_id in test[\"eeg_id\"].values:\n",
    "        img_path = f\"train_eegs/{label_id}.parquet\"\n",
    "        test_paths.append(img_path)\n",
    "\n",
    "    val_data = {\n",
    "        \"image_paths\": test_paths,\n",
    "        \"labels\": testlabels.astype('float32')}\n",
    "\n",
    "    train_data = {\n",
    "        \"image_paths\": train_paths,\n",
    "        \"labels\": trainlabels.astype('float32')}\n",
    "                                                                 \n",
    "    return train_data, val_data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fbb723-7845-4547-b3d7-f2fdc6a58a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea3ca2c9-df73-4524-bd68-892d506a0335",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path_label, val_path_label  = get_path_label(train)\n",
    "\n",
    "train_dataset = HMSHBACSpecDataset(**train_path_label)\n",
    "val_dataset = HMSHBACSpecDataset(**val_path_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd9ef221-b08c-491a-bc0c-543cf42ebe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=128, num_workers=6, pin_memory=True,  persistent_workers=True,  shuffle=True, drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=64, num_workers=2,  pin_memory=True, persistent_workers=True, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bb17fb-a821-41ca-9ebf-5a98d4f71866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "333f7d60-8fbe-4989-a07a-5270acb8dbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup basic train loop\n",
    "device='cuda:0'\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    record_step = int(len(dataloader) / 20)\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        X = batch[0].to(device)\n",
    "        y = batch[1].to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = torch.nn.functional.kl_div  (pred.softmax(1), y , reduction='batchmean') \n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % record_step == 0:\n",
    "            loss, current = loss.item(), batch_idx * len(X)\n",
    "            print(f\"Loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# validation process\n",
    "def valid(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X = batch[0].to(device)\n",
    "            y = batch[1].to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            loss += torch.nn.functional.kl_div  (pred.softmax(1), y , reduction='batchmean').item()\n",
    "    loss /= num_batches\n",
    "    print(f\"Valid Error: \\n Avg loss: {loss:>8f} \\n\" )\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c615e827-12f7-4919-b78c-94714530aff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9693556d-3b94-479c-80c2-86b72a4e1b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.9/site-packages/torch/nn/modules/conv.py:456: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1008.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 10.135015  [    0/85440]\n",
      "Loss: 9.621571  [ 4224/85440]\n",
      "Loss: 14.351191  [ 8448/85440]\n",
      "Loss: 12.452726  [12672/85440]\n",
      "Loss: 8.562347  [16896/85440]\n",
      "Loss: 11.460200  [21120/85440]\n",
      "Loss: 11.767521  [25344/85440]\n",
      "Loss: 10.154109  [29568/85440]\n",
      "Loss: 11.102566  [33792/85440]\n",
      "Loss: 10.552280  [38016/85440]\n",
      "Loss: 10.502416  [42240/85440]\n",
      "Loss: 9.232741  [46464/85440]\n",
      "Loss: 8.892500  [50688/85440]\n",
      "Loss: 11.117680  [54912/85440]\n",
      "Loss: 10.803099  [59136/85440]\n",
      "Loss: 9.676501  [63360/85440]\n",
      "Loss: 10.622193  [67584/85440]\n",
      "Loss: 9.440865  [71808/85440]\n",
      "Loss: 10.427096  [76032/85440]\n",
      "Loss: 10.394041  [80256/85440]\n",
      "Loss: 11.740202  [84480/85440]\n",
      "Valid Error: \n",
      " Avg loss: 10.999078 \n",
      "\n",
      "Loss: 10.798790  [    0/85440]\n",
      "Loss: 10.092026  [ 4224/85440]\n",
      "Loss: 10.518681  [ 8448/85440]\n",
      "Loss: 11.969891  [12672/85440]\n",
      "Loss: 10.131868  [16896/85440]\n",
      "Loss: 12.109968  [21120/85440]\n",
      "Loss: 9.401199  [25344/85440]\n",
      "Loss: 9.719956  [29568/85440]\n",
      "Loss: 8.239702  [33792/85440]\n",
      "Loss: 10.159458  [38016/85440]\n",
      "Loss: 9.966164  [42240/85440]\n",
      "Loss: 12.463900  [46464/85440]\n",
      "Loss: 11.076042  [50688/85440]\n",
      "Loss: 9.724616  [54912/85440]\n",
      "Loss: 11.921916  [59136/85440]\n",
      "Loss: 10.810435  [63360/85440]\n",
      "Loss: 13.082823  [67584/85440]\n",
      "Loss: 11.169640  [71808/85440]\n",
      "Loss: 10.848722  [76032/85440]\n",
      "Loss: 9.900599  [80256/85440]\n",
      "Loss: 10.855058  [84480/85440]\n",
      "Valid Error: \n",
      " Avg loss: 11.006630 \n",
      "\n",
      "Loss: 9.815899  [    0/85440]\n",
      "Loss: 11.558591  [ 4224/85440]\n",
      "Loss: 11.665233  [ 8448/85440]\n",
      "Loss: 10.466651  [12672/85440]\n",
      "Loss: 11.177279  [16896/85440]\n",
      "Loss: 9.454824  [21120/85440]\n",
      "Loss: 11.187197  [25344/85440]\n",
      "Loss: 11.981189  [29568/85440]\n",
      "Loss: 11.071821  [33792/85440]\n",
      "Loss: 10.923759  [38016/85440]\n",
      "Loss: 9.683126  [42240/85440]\n",
      "Loss: 8.872100  [46464/85440]\n",
      "Loss: 9.575097  [50688/85440]\n",
      "Loss: 11.279860  [54912/85440]\n",
      "Loss: 10.348435  [59136/85440]\n",
      "Loss: 10.294935  [63360/85440]\n",
      "Loss: 11.795126  [67584/85440]\n",
      "Loss: 10.304134  [71808/85440]\n",
      "Loss: 12.295620  [76032/85440]\n",
      "Loss: 10.040787  [80256/85440]\n",
      "Loss: 10.715919  [84480/85440]\n",
      "Valid Error: \n",
      " Avg loss: 11.021437 \n",
      "\n",
      "Loss: 10.437015  [    0/85440]\n",
      "Loss: 9.775795  [ 4224/85440]\n",
      "Loss: 10.218784  [ 8448/85440]\n",
      "Loss: 11.341115  [12672/85440]\n",
      "Loss: 11.445339  [16896/85440]\n",
      "Loss: 11.727448  [21120/85440]\n",
      "Loss: 12.664468  [25344/85440]\n",
      "Loss: 11.589699  [29568/85440]\n",
      "Loss: 11.223413  [33792/85440]\n",
      "Loss: 9.422752  [38016/85440]\n",
      "Loss: 12.521339  [42240/85440]\n",
      "Loss: 12.275530  [46464/85440]\n",
      "Loss: 9.088869  [50688/85440]\n",
      "Loss: 10.292109  [54912/85440]\n",
      "Loss: 10.831516  [59136/85440]\n",
      "Loss: 11.718817  [63360/85440]\n",
      "Loss: 10.915239  [67584/85440]\n",
      "Loss: 9.967038  [71808/85440]\n",
      "Loss: 9.425707  [76032/85440]\n",
      "Loss: 11.534747  [80256/85440]\n",
      "Loss: 11.022882  [84480/85440]\n",
      "Valid Error: \n",
      " Avg loss: 10.987993 \n",
      "\n",
      "Loss: 11.879003  [    0/85440]\n",
      "Loss: 11.160770  [ 4224/85440]\n",
      "Loss: 9.959308  [ 8448/85440]\n",
      "Loss: 13.802284  [12672/85440]\n",
      "Loss: 8.481407  [16896/85440]\n",
      "Loss: 11.980560  [21120/85440]\n",
      "Loss: 11.254995  [25344/85440]\n",
      "Loss: 11.658814  [29568/85440]\n",
      "Loss: 13.093230  [33792/85440]\n",
      "Loss: 10.807764  [38016/85440]\n",
      "Loss: 11.461495  [42240/85440]\n",
      "Loss: 10.064867  [46464/85440]\n",
      "Loss: 10.724250  [50688/85440]\n",
      "Loss: 10.015312  [54912/85440]\n",
      "Loss: 12.330582  [59136/85440]\n",
      "Loss: 11.732493  [63360/85440]\n",
      "Loss: 12.110582  [67584/85440]\n",
      "Loss: 9.220628  [71808/85440]\n",
      "Loss: 9.104344  [76032/85440]\n",
      "Loss: 10.929174  [80256/85440]\n",
      "Loss: 11.817777  [84480/85440]\n",
      "Valid Error: \n",
      " Avg loss: 10.982570 \n",
      "\n",
      "Loss: 12.823065  [    0/85440]\n",
      "Loss: 12.145336  [ 4224/85440]\n",
      "Loss: 8.439342  [ 8448/85440]\n",
      "Loss: 10.555336  [12672/85440]\n",
      "Loss: 11.911949  [16896/85440]\n",
      "Loss: 11.223881  [21120/85440]\n",
      "Loss: 9.394235  [25344/85440]\n",
      "Loss: 10.211123  [29568/85440]\n",
      "Loss: 12.034056  [33792/85440]\n",
      "Loss: 9.835262  [38016/85440]\n",
      "Loss: 10.451263  [42240/85440]\n",
      "Loss: 9.252735  [46464/85440]\n",
      "Loss: 10.797533  [50688/85440]\n",
      "Loss: 10.149374  [54912/85440]\n",
      "Loss: 11.364254  [59136/85440]\n",
      "Loss: 11.895084  [63360/85440]\n",
      "Loss: 9.864533  [67584/85440]\n",
      "Loss: 10.423579  [71808/85440]\n",
      "Loss: 10.608337  [76032/85440]\n",
      "Loss: 10.585274  [80256/85440]\n",
      "Loss: 11.195718  [84480/85440]\n",
      "Valid Error: \n",
      " Avg loss: 10.975135 \n",
      "\n",
      "Loss: 9.446196  [    0/85440]\n",
      "Loss: 10.142509  [ 4224/85440]\n",
      "Loss: 10.028099  [ 8448/85440]\n",
      "Loss: 11.192860  [12672/85440]\n",
      "Loss: 12.545699  [16896/85440]\n",
      "Loss: 10.444187  [21120/85440]\n",
      "Loss: 10.631166  [25344/85440]\n",
      "Loss: 10.545933  [29568/85440]\n",
      "Loss: 10.913356  [33792/85440]\n",
      "Loss: 10.166278  [38016/85440]\n",
      "Loss: 11.676286  [42240/85440]\n",
      "Loss: 9.378554  [46464/85440]\n",
      "Loss: 10.636795  [50688/85440]\n",
      "Loss: 11.995234  [54912/85440]\n",
      "Loss: 9.741318  [59136/85440]\n",
      "Loss: 8.687473  [63360/85440]\n",
      "Loss: 12.885192  [67584/85440]\n",
      "Loss: 11.497558  [71808/85440]\n",
      "Loss: 10.442054  [76032/85440]\n",
      "Loss: 11.502361  [80256/85440]\n",
      "Loss: 11.794770  [84480/85440]\n",
      "Valid Error: \n",
      " Avg loss: 10.974170 \n",
      "\n",
      "Loss: 11.578976  [    0/85440]\n",
      "Loss: 12.557740  [ 4224/85440]\n",
      "Loss: 12.471851  [ 8448/85440]\n",
      "Loss: 10.210554  [12672/85440]\n",
      "Loss: 9.570667  [16896/85440]\n",
      "Loss: 12.460730  [21120/85440]\n",
      "Loss: 11.169167  [25344/85440]\n",
      "Loss: 11.424385  [29568/85440]\n",
      "Loss: 10.832909  [33792/85440]\n",
      "Loss: 10.496585  [38016/85440]\n",
      "Loss: 9.236538  [42240/85440]\n",
      "Loss: 12.241337  [46464/85440]\n",
      "Loss: 10.459394  [50688/85440]\n",
      "Loss: 10.595993  [54912/85440]\n",
      "Loss: 10.042187  [59136/85440]\n",
      "Loss: 11.359806  [63360/85440]\n",
      "Loss: 10.281261  [67584/85440]\n",
      "Loss: 10.482361  [71808/85440]\n",
      "Loss: 11.854660  [76032/85440]\n",
      "Loss: 9.593195  [80256/85440]\n",
      "Loss: 9.605238  [84480/85440]\n",
      "Valid Error: \n",
      " Avg loss: 10.976887 \n",
      "\n",
      "Loss: 10.483839  [    0/85440]\n",
      "Loss: 11.334766  [ 4224/85440]\n",
      "Loss: 12.709391  [ 8448/85440]\n",
      "Loss: 13.124117  [12672/85440]\n",
      "Loss: 11.190407  [16896/85440]\n",
      "Loss: 10.941235  [21120/85440]\n",
      "Loss: 10.268609  [25344/85440]\n",
      "Loss: 11.626692  [29568/85440]\n",
      "Loss: 11.427986  [33792/85440]\n",
      "Loss: 10.485199  [38016/85440]\n",
      "Loss: 12.665021  [42240/85440]\n",
      "Loss: 10.032872  [46464/85440]\n",
      "Loss: 9.637653  [50688/85440]\n",
      "Loss: 10.767284  [54912/85440]\n",
      "Loss: 10.316761  [59136/85440]\n",
      "Loss: 11.212831  [63360/85440]\n",
      "Loss: 10.445776  [67584/85440]\n",
      "Loss: 9.951006  [71808/85440]\n",
      "Loss: 10.396408  [76032/85440]\n",
      "Loss: 9.669851  [80256/85440]\n",
      "Loss: 11.122503  [84480/85440]\n",
      "Valid Error: \n",
      " Avg loss: 10.972554 \n",
      "\n",
      "Loss: 10.824678  [    0/85440]\n",
      "Loss: 11.403227  [ 4224/85440]\n",
      "Loss: 11.523693  [ 8448/85440]\n",
      "Loss: 12.873604  [12672/85440]\n",
      "Loss: 11.008940  [16896/85440]\n",
      "Loss: 11.936459  [21120/85440]\n",
      "Loss: 11.177465  [25344/85440]\n",
      "Loss: 9.589841  [29568/85440]\n",
      "Loss: 11.982887  [33792/85440]\n",
      "Loss: 8.588491  [38016/85440]\n",
      "Loss: 10.244781  [42240/85440]\n",
      "Loss: 9.440559  [46464/85440]\n",
      "Loss: 11.462152  [50688/85440]\n",
      "Loss: 9.489717  [54912/85440]\n",
      "Loss: 11.387825  [59136/85440]\n",
      "Loss: 9.546648  [63360/85440]\n",
      "Loss: 9.874293  [67584/85440]\n",
      "Loss: 11.064610  [71808/85440]\n",
      "Loss: 11.574966  [76032/85440]\n",
      "Loss: 10.974886  [80256/85440]\n",
      "Loss: 11.778642  [84480/85440]\n",
      "Valid Error: \n",
      " Avg loss: 10.966073 \n",
      "\n",
      "Loss: 10.657783  [    0/85440]\n",
      "Loss: 11.943853  [ 4224/85440]\n",
      "Loss: 11.420958  [ 8448/85440]\n",
      "Loss: 11.180569  [12672/85440]\n",
      "Loss: 11.105334  [16896/85440]\n",
      "Loss: 10.392747  [21120/85440]\n",
      "Loss: 8.803154  [25344/85440]\n",
      "Loss: 9.682055  [29568/85440]\n",
      "Loss: 10.590528  [33792/85440]\n",
      "Loss: 11.509523  [38016/85440]\n",
      "Loss: 12.259853  [42240/85440]\n",
      "Loss: 10.027677  [46464/85440]\n",
      "Loss: 10.713722  [50688/85440]\n",
      "Loss: 11.228490  [54912/85440]\n",
      "Loss: 9.934569  [59136/85440]\n",
      "Loss: 11.259491  [63360/85440]\n",
      "Loss: 8.592118  [67584/85440]\n",
      "Loss: 11.630962  [71808/85440]\n",
      "Loss: 12.216093  [76032/85440]\n",
      "Loss: 10.216682  [80256/85440]\n",
      "Loss: 10.053652  [84480/85440]\n",
      "Valid Error: \n",
      " Avg loss: 10.964657 \n",
      "\n",
      "Loss: 11.441608  [    0/85440]\n",
      "Loss: 11.923366  [ 4224/85440]\n",
      "Loss: 10.940637  [ 8448/85440]\n",
      "Loss: 11.891294  [12672/85440]\n",
      "Loss: 11.127191  [16896/85440]\n",
      "Loss: 11.640430  [21120/85440]\n",
      "Loss: 10.301976  [25344/85440]\n",
      "Loss: 10.975606  [29568/85440]\n",
      "Loss: 10.381460  [33792/85440]\n",
      "Loss: 9.164743  [38016/85440]\n",
      "Loss: 10.939028  [42240/85440]\n",
      "Loss: 10.313771  [46464/85440]\n",
      "Loss: 11.903269  [50688/85440]\n",
      "Loss: 14.401430  [54912/85440]\n",
      "Loss: 9.354157  [59136/85440]\n",
      "Loss: 10.022274  [63360/85440]\n",
      "Loss: 11.907248  [67584/85440]\n",
      "Loss: 10.223951  [71808/85440]\n",
      "Loss: 11.484262  [76032/85440]\n",
      "Loss: 8.289771  [80256/85440]\n",
      "Loss: 10.201563  [84480/85440]\n",
      "Valid Error: \n",
      " Avg loss: 10.954639 \n",
      "\n",
      "Loss: 11.227283  [    0/85440]\n",
      "Loss: 9.123959  [ 4224/85440]\n",
      "Loss: 12.598565  [ 8448/85440]\n",
      "Loss: 10.975769  [12672/85440]\n",
      "Loss: 8.895855  [16896/85440]\n",
      "Loss: 11.815606  [21120/85440]\n",
      "Loss: 11.839549  [25344/85440]\n",
      "Loss: 9.031996  [29568/85440]\n",
      "Loss: 11.185645  [33792/85440]\n",
      "Loss: 10.256275  [38016/85440]\n",
      "Loss: 10.792595  [42240/85440]\n",
      "Loss: 11.143728  [46464/85440]\n",
      "Loss: 9.295931  [50688/85440]\n",
      "Loss: 10.808790  [54912/85440]\n",
      "Loss: 11.283613  [59136/85440]\n",
      "Loss: 10.041715  [63360/85440]\n",
      "Loss: 9.438417  [67584/85440]\n",
      "Loss: 10.382273  [71808/85440]\n",
      "Loss: 9.976532  [76032/85440]\n",
      "Loss: 11.145480  [80256/85440]\n",
      "Loss: 8.112244  [84480/85440]\n",
      "Valid Error: \n",
      " Avg loss: 10.951683 \n",
      "\n",
      "Loss: 10.242189  [    0/85440]\n",
      "Loss: 10.678981  [ 4224/85440]\n",
      "Loss: 10.953661  [ 8448/85440]\n",
      "Loss: 9.413362  [12672/85440]\n",
      "Loss: 14.051225  [16896/85440]\n",
      "Loss: 12.840308  [21120/85440]\n",
      "Loss: 11.408255  [25344/85440]\n",
      "Loss: 12.391539  [29568/85440]\n",
      "Loss: 11.282363  [33792/85440]\n",
      "Loss: 10.235004  [38016/85440]\n",
      "Loss: 10.655456  [42240/85440]\n",
      "Loss: 12.083694  [46464/85440]\n",
      "Loss: 10.871399  [50688/85440]\n",
      "Loss: 9.013299  [54912/85440]\n",
      "Loss: 10.432789  [59136/85440]\n",
      "Loss: 9.694710  [63360/85440]\n",
      "Loss: 11.109898  [67584/85440]\n",
      "Loss: 8.989907  [71808/85440]\n",
      "Loss: 10.158379  [76032/85440]\n",
      "Loss: 10.865438  [80256/85440]\n",
      "Loss: 10.890046  [84480/85440]\n",
      "Valid Error: \n",
      " Avg loss: 10.971922 \n",
      "\n",
      "Loss: 10.770615  [    0/85440]\n",
      "Loss: 10.875971  [ 4224/85440]\n",
      "Loss: 8.271049  [ 8448/85440]\n",
      "Loss: 11.073772  [12672/85440]\n",
      "Loss: 11.492921  [16896/85440]\n",
      "Loss: 9.887257  [21120/85440]\n",
      "Loss: 9.448416  [25344/85440]\n",
      "Loss: 12.317921  [29568/85440]\n",
      "Loss: 10.153111  [33792/85440]\n",
      "Loss: 9.464991  [38016/85440]\n",
      "Loss: 9.496422  [42240/85440]\n",
      "Loss: 11.386438  [46464/85440]\n",
      "Loss: 11.861650  [50688/85440]\n",
      "Loss: 12.067057  [54912/85440]\n",
      "Loss: 12.536533  [59136/85440]\n",
      "Loss: 9.207789  [63360/85440]\n",
      "Loss: 10.817062  [67584/85440]\n",
      "Loss: 11.014567  [71808/85440]\n",
      "Loss: 11.667104  [76032/85440]\n",
      "Loss: 9.984430  [80256/85440]\n",
      "Loss: 10.205739  [84480/85440]\n",
      "Valid Error: \n",
      " Avg loss: 10.949509 \n",
      "\n",
      "Loss: 11.086933  [    0/85440]\n",
      "Loss: 11.379413  [ 4224/85440]\n",
      "Loss: 12.567001  [ 8448/85440]\n",
      "Loss: 10.529320  [12672/85440]\n",
      "Loss: 11.891719  [16896/85440]\n",
      "Loss: 10.601704  [21120/85440]\n",
      "Loss: 11.604753  [25344/85440]\n",
      "Loss: 11.435164  [29568/85440]\n",
      "Loss: 9.785717  [33792/85440]\n",
      "Loss: 13.847643  [38016/85440]\n",
      "Loss: 11.148529  [42240/85440]\n",
      "Loss: 10.389479  [46464/85440]\n",
      "Loss: 10.156880  [50688/85440]\n",
      "Loss: 9.696573  [54912/85440]\n",
      "Loss: 9.526054  [59136/85440]\n",
      "Loss: 10.652606  [63360/85440]\n",
      "Loss: 11.931387  [67584/85440]\n",
      "Loss: 11.040749  [71808/85440]\n",
      "Loss: 12.482177  [76032/85440]\n",
      "Loss: 10.139293  [80256/85440]\n",
      "Loss: 9.213867  [84480/85440]\n",
      "Valid Error: \n",
      " Avg loss: 10.972467 \n",
      "\n",
      "Loss: 14.719004  [    0/85440]\n",
      "Loss: 11.718296  [ 4224/85440]\n",
      "Loss: 10.941982  [ 8448/85440]\n",
      "Loss: 10.602896  [12672/85440]\n",
      "Loss: 10.349145  [16896/85440]\n",
      "Loss: 11.027842  [21120/85440]\n",
      "Loss: 12.050806  [25344/85440]\n",
      "Loss: 8.954998  [29568/85440]\n",
      "Loss: 10.812241  [33792/85440]\n",
      "Loss: 10.312457  [38016/85440]\n",
      "Loss: 11.540332  [42240/85440]\n",
      "Loss: 10.688751  [46464/85440]\n",
      "Loss: 11.960644  [50688/85440]\n",
      "Loss: 10.224325  [54912/85440]\n",
      "Loss: 10.421552  [59136/85440]\n",
      "Loss: 12.144059  [63360/85440]\n",
      "Loss: 11.074233  [67584/85440]\n",
      "Loss: 9.420116  [71808/85440]\n",
      "Loss: 10.354772  [76032/85440]\n",
      "Loss: 9.158879  [80256/85440]\n",
      "Loss: 10.535366  [84480/85440]\n",
      "Valid Error: \n",
      " Avg loss: 10.957356 \n",
      "\n",
      "Loss: 10.837587  [    0/85440]\n",
      "Loss: 11.820363  [ 4224/85440]\n",
      "Loss: 9.186724  [ 8448/85440]\n",
      "Loss: 10.562326  [12672/85440]\n",
      "Loss: 9.898066  [16896/85440]\n",
      "Loss: 9.884563  [21120/85440]\n",
      "Loss: 10.548279  [25344/85440]\n",
      "Loss: 11.179267  [29568/85440]\n",
      "Loss: 11.806151  [33792/85440]\n",
      "Loss: 11.336946  [38016/85440]\n",
      "Loss: 12.095320  [42240/85440]\n",
      "Loss: 10.555415  [46464/85440]\n",
      "Loss: 11.567631  [50688/85440]\n",
      "Loss: 10.557371  [54912/85440]\n",
      "Loss: 9.826941  [59136/85440]\n",
      "Loss: 10.332598  [63360/85440]\n",
      "Loss: 10.802600  [67584/85440]\n",
      "Loss: 11.112737  [71808/85440]\n",
      "Loss: 10.226917  [76032/85440]\n",
      "Loss: 9.274229  [80256/85440]\n",
      "Loss: 11.839379  [84480/85440]\n",
      "Valid Error: \n",
      " Avg loss: 10.938957 \n",
      "\n",
      "Loss: 11.414359  [    0/85440]\n",
      "Loss: 11.516309  [ 4224/85440]\n",
      "Loss: 11.673668  [ 8448/85440]\n",
      "Loss: 9.892514  [12672/85440]\n",
      "Loss: 11.037924  [16896/85440]\n",
      "Loss: 11.280154  [21120/85440]\n",
      "Loss: 11.290224  [25344/85440]\n",
      "Loss: 10.755306  [29568/85440]\n",
      "Loss: 10.195292  [33792/85440]\n",
      "Loss: 11.329947  [38016/85440]\n",
      "Loss: 9.500912  [42240/85440]\n",
      "Loss: 9.580322  [46464/85440]\n",
      "Loss: 11.627413  [50688/85440]\n",
      "Loss: 10.747307  [54912/85440]\n",
      "Loss: 10.357311  [59136/85440]\n",
      "Loss: 9.120285  [63360/85440]\n",
      "Loss: 10.819125  [67584/85440]\n",
      "Loss: 11.683609  [71808/85440]\n",
      "Loss: 11.295300  [76032/85440]\n",
      "Loss: 10.034485  [80256/85440]\n",
      "Loss: 10.435656  [84480/85440]\n",
      "Valid Error: \n",
      " Avg loss: 10.935427 \n",
      "\n",
      "Loss: 9.915702  [    0/85440]\n",
      "Loss: 9.252890  [ 4224/85440]\n",
      "Loss: 11.393511  [ 8448/85440]\n",
      "Loss: 11.859647  [12672/85440]\n",
      "Loss: 10.200547  [16896/85440]\n",
      "Loss: 11.885455  [21120/85440]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = ATCNet(\n",
    "    in_channels=1,\n",
    "    num_classes=6,\n",
    "    num_windows=8, \n",
    "    num_electrodes=4,\n",
    "    chunk_size=10000,\n",
    ")\n",
    "\n",
    "model = model.to('cuda:0')\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=9e-4\n",
    ")\n",
    "epochs=1000\n",
    "\n",
    "best_val_loss=100. \n",
    "for t in range(epochs):\n",
    "    train_loss = train(train_loader, model, None, optimizer) \n",
    "    valid_loss = valid(val_loader, model, None)\n",
    " \n",
    "    if valid_loss < best_val_loss:\n",
    "        _ = torch.save(model.state_dict(),f'/storage/modeling/atcn/model{t}.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54971e38-7d2c-4d72-929f-25f6fd5524f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d085e59d-4881-4a65-9087-716d05c65e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2,3.]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebdab41-09bf-4275-9ee6-220416517bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.kl_div(a.softmax(1), torch.tensor([0.1,0.2,0.7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c91042-623f-45ff-b224-862d20ddd2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b,v in train_loader: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf92f13-136e-4097-8222-22bddd619ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
